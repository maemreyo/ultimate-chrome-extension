
// src/modules/ai/enhanced-service.ts
import { AIService } from '../ai-service'
import {
  RetryMechanism,
  RequestQueue,
  TokenManager,
  CostOptimizer,
  ContextWindowManager,
  PerformanceMonitor,
  ConnectionPool,
  EnhancedErrorHandler,
  DebugMode
} from './enhancements'

export class EnhancedAIService extends AIService {
  private retryMechanism = new RetryMechanism()
  private requestQueue = new RequestQueue({
    concurrency: 3,
    onQueueChange: (size) => this.debugMode.log('queue', `Queue size: ${size}`)
  })
  private tokenManager = new TokenManager()
  private costOptimizer = new CostOptimizer()
  private contextManager = new ContextWindowManager()
  private performanceMonitor = new PerformanceMonitor()
  private connectionPool = new ConnectionPool()
  private errorHandler = new EnhancedErrorHandler()
  private debugMode = new DebugMode()

  async generateText(prompt: string, options?: GenerateOptions): Promise<string> {
    const operationId = `generateText-${Date.now()}`

    return this.requestQueue.add(
      async () => {
        this.performanceMonitor.startMeasure(operationId, {
          provider: this.config?.provider,
          tokens: this.tokenManager.getTokenCount(prompt, this.config?.model || 'gpt-3.5-turbo')
        })

        try {
          // Use retry mechanism
          const result = await this.retryMechanism.executeWithRetry(
            () => super.generateText(prompt, options),
            {
              onRetry: (error, attempt) => {
                this.debugMode.log('retry', `Retrying after error: ${error.message}`, { attempt })
              }
            }
          )

          const report = this.performanceMonitor.endMeasure(operationId)
          if (report?.bottlenecks.length > 0) {
            this.debugMode.log('performance', 'Performance issues detected', report)
          }

          return result
        } catch (error) {
          const enhancedError = this.errorHandler.handleError(error, {
            provider: this.config?.provider || 'unknown',
            operation: 'generateText',
            attempt: 1
          })
          throw enhancedError
        }
      },
      RequestQueue.PRIORITY.NORMAL
    )
  }

  async optimizeRequest(prompt: string, requirements?: any) {
    const optimal = await this.costOptimizer.selectOptimalProvider(prompt, requirements)

    this.debugMode.log('optimization', 'Selected optimal provider', optimal)

    // Temporarily switch to optimal provider
    const originalProvider = this.config?.provider
    if (this.config) {
      this.config.provider = optimal.provider as any
      this.config.model = optimal.model
    }

    try {
      return await this.generateText(prompt)
    } finally {
      // Restore original provider
      if (this.config && originalProvider) {
        this.config.provider = originalProvider
      }
    }
  }

  enableDebugMode(options?: any) {
    this.debugMode.enable(options)
  }

  disableDebugMode() {
    this.debugMode.disable()
  }

  getDebugLogs(filter?: any) {
    return this.debugMode.getEvents(filter)
  }

  getPerformanceStats() {
    return {
      averageMetrics: this.performanceMonitor.getAverageMetrics(),
      trends: this.performanceMonitor.getPerformanceTrends(),
      connectionPool: this.connectionPool.getPoolStats(),
      queue: this.requestQueue.getQueueStatus(),
      errors: this.errorHandler.getErrorStats()
    }
  }
}

// Export the enhanced service
export const enhancedAIService = new EnhancedAIService()
